{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2405062,"sourceType":"datasetVersion","datasetId":1376382}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/harishlakshman/cs5661-project?scriptVersionId=234961814\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nimport pytorch_lightning as pl #to speed up execution\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import MNIST\n\nPATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\nBATCH_SIZE = 256 if torch.cuda.is_available() else 64\nNUM_WORKERS = int(os.cpu_count() / 2) #check if we have GPU's or multiple CPU cores\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"vishalkundar/gandata20\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T00:25:26.238008Z","iopub.execute_input":"2025-04-20T00:25:26.23856Z","iopub.status.idle":"2025-04-20T00:25:40.273207Z","shell.execute_reply.started":"2025-04-20T00:25:26.238494Z","shell.execute_reply":"2025-04-20T00:25:40.272046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\n# Sample data\ndata = {'a': 1, 'b': [2, 3, 4], 'c': 'hello'}\n\n# Pickling (serializing) and saving to a file\nwith open('data.pickle', 'wb') as file:\n    pickle.dump(data, file)\n\n# Unpickling (deserializing) from a file\nwith open('data.pickle', 'rb') as file:\n    loaded_data = pickle.load(file)\n\nprint(loaded_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T00:26:46.710371Z","iopub.execute_input":"2025-04-20T00:26:46.710849Z","iopub.status.idle":"2025-04-20T00:26:46.719474Z","shell.execute_reply.started":"2025-04-20T00:26:46.710813Z","shell.execute_reply":"2025-04-20T00:26:46.718005Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def displaySingleImage(X_train, y_train):\n    plt.imshow(X_train[0], cmap=\"gray\")\n    plt.title(f\"Label: {y_train[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T03:51:58.428314Z","iopub.execute_input":"2025-04-14T03:51:58.428631Z","iopub.status.idle":"2025-04-14T03:51:58.433229Z","shell.execute_reply.started":"2025-04-14T03:51:58.428605Z","shell.execute_reply":"2025-04-14T03:51:58.431823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_type(variable):\n    if isinstance(variable, np.ndarray):\n        return \"NumPy array\"\n    elif isinstance(variable, pd.DataFrame):\n        return \"Pandas DataFrame\"\n    else:\n        return \"Neither NumPy array nor Pandas DataFrame\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T03:52:00.978413Z","iopub.execute_input":"2025-04-14T03:52:00.978762Z","iopub.status.idle":"2025-04-14T03:52:00.983309Z","shell.execute_reply.started":"2025-04-14T03:52:00.978737Z","shell.execute_reply":"2025-04-14T03:52:00.981944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"/kaggle/input/gandata20/birds/train/64images.pickle\", \"rb\") as file:\n    low_res_Xtrain = pickle.load(file)\nfile.close()    \nprint(low_res_Xtrain[0])    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T00:26:51.791701Z","iopub.execute_input":"2025-04-20T00:26:51.792093Z","iopub.status.idle":"2025-04-20T00:26:52.883963Z","shell.execute_reply.started":"2025-04-20T00:26:51.792065Z","shell.execute_reply":"2025-04-20T00:26:52.882326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"a = low_res_Xtrain['labels']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T00:29:23.495863Z","iopub.execute_input":"2025-04-20T00:29:23.49625Z","iopub.status.idle":"2025-04-20T00:29:23.522247Z","shell.execute_reply.started":"2025-04-20T00:29:23.496212Z","shell.execute_reply":"2025-04-20T00:29:23.520749Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"   \n#print(low_res_ytrain[0])\n# Try loading with latin1 encoding\ntry:\n    with open(\"/kaggle/input/gandata20/birds/train/char-CNN-RNN-embeddings.pickle\", 'rb') as f:\n        low_res_ytrain = pickle.load(f, encoding='latin1')\nexcept UnicodeDecodeError:\n    # If latin1 fails, try bytes encoding\n    with open(\"/kaggle/input/gandata20/birds/train/char-CNN-RNN-embeddings.pickle\", 'rb') as f:\n        low_res_ytrain = pickle.load(f, encoding='bytes')\n        # Depending on the data structure, you might need to decode the keys and values\n        # Example:\n        # data = {k.decode('utf-8'): v.decode('utf-8') for k, v in data.items()}\nf.close()   \n\nprint(low_res_ytrain[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T06:08:41.220265Z","iopub.execute_input":"2025-03-20T06:08:41.22063Z","iopub.status.idle":"2025-03-20T06:08:54.022907Z","shell.execute_reply.started":"2025-03-20T06:08:41.220602Z","shell.execute_reply":"2025-03-20T06:08:54.021949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(low_res_ytrain))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T06:12:18.063696Z","iopub.execute_input":"2025-03-20T06:12:18.064043Z","iopub.status.idle":"2025-03-20T06:12:18.069558Z","shell.execute_reply.started":"2025-03-20T06:12:18.064015Z","shell.execute_reply":"2025-03-20T06:12:18.068363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(low_res_ytrain)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T06:13:31.644661Z","iopub.execute_input":"2025-03-20T06:13:31.645023Z","iopub.status.idle":"2025-03-20T06:13:31.651422Z","shell.execute_reply.started":"2025-03-20T06:13:31.644993Z","shell.execute_reply":"2025-03-20T06:13:31.650454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"low_res_ytrain_np = np.array(low_res_ytrain)\nlow_res_ytrain_np.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T06:17:58.188209Z","iopub.execute_input":"2025-03-20T06:17:58.188641Z","iopub.status.idle":"2025-03-20T06:17:58.353764Z","shell.execute_reply.started":"2025-03-20T06:17:58.188611Z","shell.execute_reply":"2025-03-20T06:17:58.352762Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"first_image = low_res_ytrain_np[0]\nfirst_caption_embedding = first_image[0]\nprint(\"First image, first caption embedding: \", first_caption_embedding)\nlow_res_ytrain_np_flat = low_res_ytrain_np.reshape(8855,-1)\ndf = pd.DataFrame(low_res_ytrain_np_flat)\ndf.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T06:30:41.690141Z","iopub.execute_input":"2025-03-20T06:30:41.690506Z","iopub.status.idle":"2025-03-20T06:30:41.724677Z","shell.execute_reply.started":"2025-03-20T06:30:41.690475Z","shell.execute_reply":"2025-03-20T06:30:41.723657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('64images type : ', check_type(low_res_Xtrain))\nprint('length of list: ', len(low_res_Xtrain))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T03:54:03.386304Z","iopub.execute_input":"2025-04-14T03:54:03.386619Z","iopub.status.idle":"2025-04-14T03:54:03.391407Z","shell.execute_reply.started":"2025-04-14T03:54:03.386596Z","shell.execute_reply":"2025-04-14T03:54:03.390395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(low_res_Xtrain[0], cmap=\"gray\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T03:54:08.020441Z","iopub.execute_input":"2025-04-14T03:54:08.020763Z","iopub.status.idle":"2025-04-14T03:54:08.236789Z","shell.execute_reply.started":"2025-04-14T03:54:08.020735Z","shell.execute_reply":"2025-04-14T03:54:08.235796Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create a class that inherits from Pytorch lightning data module.\n## Responsible for creating data loaders for training, validation & test set","metadata":{}},{"cell_type":"code","source":"class PickleImageDataset(Dataset):\n    def __init__(self, pickle_path, tranform=None):\n        with open(pickle_path, 'rb') as f:\n            data = pickle.load(f)\n        self.images = data\n        self.transform = transform\n\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img = self.images[idx]\n\n        #convert to tensor if not already\n        if isinstance(img, np.ndarray):\n            img = torch.tensor(img, dtype=torch.float32).permute(2,0,1) / 255.0\n\n        if self.transform:\n            img = self.transform(img)\n        return img\n\nclass BIRDSDataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        pickle_path_train,\n        pickle_path_test,\n        batch_size: int = BATCH_SIZE,\n        val_split=0.2,\n        num_workers: int = NUM_WORKERS\n    ):\n        super().__init__()\n        self.pickle_path_train = pickle_path_train\n        self.pickle_path_test =  pickle_path_test #initalize class data with dataset\n        self.batch_size = batch_size #initalize class data batch size\n        self.val_split = val_split\n        self.num_workers = num_workers #assigns cpu cores\n\n        #defines class tranformation methods\n        #Tensors are similar to NumPy arrays, but tensors have accelerator support. \n        #Tensors are optimized for automatic differentiation.\n        self.transform = transforms.Compose(\n            [\n                transforms.ToTensor(), #transforms images to tensors.\n                transforms.Normalize((0.1307,), (0.3081,)), #tranformed data is normalized. Ex: 0.1307 = mean; 0.3081 = std deviation\n            ]\n        )\n\n        self.dims = (1, 28, 28)\n        self.num_classes = 10\n\n        #def prepare_data(self):\n        # download\n        #MNIST(self.data_dir, train=True, download=True) #download training data\n        #MNIST(self.data_dir, train=False, download=True)#download testing data\n\n    # Assign train/val datasets for use in dataloaders\n    def setup(self, stage=None):\n        \n        if stage == \"fit\" or stage is None:\n            # Assign train/val datasets for use in dataloaders\n            dataset = PickleImageDataset(self.pickle_path_train, transform=self.transform)\n            val_size = int(len(dataset) * self.val_split)\n            train_size = len(dataset) - val_size\n            self.train_dataset, self.val_dataset = random_split(dataset, [train_size, val_size])\n            \n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            dataset = PickleImageDataset(self.pickle_path_test, transform=self.transform)\n            val_size = 0\n            test_size = len(dataset) - val_size\n            self.test_dataset, x = random_split(dataset, [test_size, val_size])\n\n    #Training\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n        )\n    #Validation\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n    #Testing\n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=self.num_workers)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Download the datset","metadata":{}},{"cell_type":"markdown","source":"## split the data into training & validation","metadata":{}},{"cell_type":"markdown","source":"### BUILD THE GENERATOR\n#### Generate fake data from random noise","metadata":{}},{"cell_type":"code","source":"def build_generator():","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### BUILD THE DISCRIMINATOR\n#### Distinguish between real and fake data","metadata":{}},{"cell_type":"code","source":"def build_discriminator():","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ","metadata":{}}]}