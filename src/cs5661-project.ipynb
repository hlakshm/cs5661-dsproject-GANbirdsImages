{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2405062,"sourceType":"datasetVersion","datasetId":1376382}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/harishlakshman/cs5661-project?scriptVersionId=239026677\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n!pip install pytorch-lightning\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nimport pytorch_lightning as pl #to speed up execution\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import datasets\nfrom tqdm import tqdm\n\nPATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\nBATCH_SIZE = 256 if torch.cuda.is_available() else 64\nNUM_WORKERS = int(os.cpu_count() / 2) #check if we have GPU's or multiple CPU cores\n\nAVAIL_GPUS = min(1, torch.cuda.device_count())\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n%matplotlib inline\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"vishalkundar/gandata20\")\n\nprint(\"Path to dataset files:\", path)\ntrainingDataPath = path + '/birds/train/64images.pickle'\ntestindDataPath = path + '/birds/test/64images.pickle'\nprint(\"Path to training dataset files:\", trainingDataPath)\nprint(\"Path to testing dataset files:\", testindDataPath)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\n# Sample data\ndata = {'a': 1, 'b': [2, 3, 4], 'c': 'hello'}\n\n# Pickling (serializing) and saving to a file\nwith open('data.pickle', 'wb') as file:\n    pickle.dump(data, file)\n\n# Unpickling (deserializing) from a file\nwith open('data.pickle', 'rb') as file:\n    loaded_data = pickle.load(file)\n\nprint(loaded_data)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.58Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def displaySingleImage(X_train, y_train):\n    plt.imshow(X_train[0], cmap=\"gray\")\n    plt.title(f\"Label: {y_train[0]}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.58Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_type(variable):\n    if isinstance(variable, np.ndarray):\n        return \"NumPy array\"\n    elif isinstance(variable, pd.DataFrame):\n        return \"Pandas DataFrame\"\n    else:\n        return \"Neither NumPy array nor Pandas DataFrame\"","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.58Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# \"/kaggle/input/gandata20/birds/train/64images.pickle\"\nwith open(trainingDataPath, \"rb\") as file:\n    low_res_Xtrain = pickle.load(file)\nfile.close()    \nprint(low_res_Xtrain[0])    ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.58Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"   \n#print(low_res_ytrain[0])\n# Try loading with latin1 encoding\ntry:\n    with open(\"/kaggle/input/gandata20/birds/train/char-CNN-RNN-embeddings.pickle\", 'rb') as f:\n        low_res_ytrain = pickle.load(f, encoding='latin1')\nexcept UnicodeDecodeError:\n    # If latin1 fails, try bytes encoding\n    with open(\"/kaggle/input/gandata20/birds/train/char-CNN-RNN-embeddings.pickle\", 'rb') as f:\n        low_res_ytrain = pickle.load(f, encoding='bytes')\n        # Depending on the data structure, you might need to decode the keys and values\n        # Example:\n        # data = {k.decode('utf-8'): v.decode('utf-8') for k, v in data.items()}\nf.close()   \n\nprint(low_res_ytrain[0])\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.58Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(low_res_Xtrain))\nprint(len(low_res_ytrain))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.58Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(low_res_Xtrain)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.58Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"low_res_ytrain_np = np.array(low_res_ytrain)\nlow_res_ytrain_np.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.58Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"first_image = low_res_ytrain_np[0]\nfirst_caption_embedding = first_image[0]\nprint(\"First image, first caption embedding: \", first_caption_embedding)\nlow_res_ytrain_np_flat = low_res_ytrain_np.reshape(8855,-1)\ndf = pd.DataFrame(low_res_ytrain_np_flat)\ndf.head(5)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.58Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"low_res_Xtrain_np = np.array(low_res_Xtrain)\nlow_res_Xtrain_np.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.58Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('64images type : ', check_type(low_res_Xtrain))\nprint('length of list: ', len(low_res_Xtrain))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.58Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(low_res_Xtrain[0], cmap=\"gray\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.58Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Download the datset","metadata":{}},{"cell_type":"markdown","source":"## split the data into training & validation","metadata":{}},{"cell_type":"markdown","source":"## Create a class that inherits from Pytorch lightning data module.\n## Responsible for creating data loaders for training, validation & test set","metadata":{}},{"cell_type":"code","source":"class PickleImageDataset(Dataset):\n    def __init__(self, pickle_path, transform=None):\n        with open(pickle_path, 'rb') as f:\n            self.images = pickle.load(f)\n        self.images = [img for img in self.images if img is not None]\n        self.transform = transform\n\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img = self.images[idx]\n\n        #convert to tensor if not already\n        if isinstance(img, np.ndarray):\n            img = torch.from_numpy(img).float() / 255.0\n            if img.shape[-1] == 3:\n                img = img.permute(2,0,1)\n            #img = torch.tensor(img, dtype=torch.float32).permute(2,0,1) / 255.0\n            #img = F.interpolate(img.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n        if self.transform:\n            img = self.transform(img)\n        return img\n\nclass BIRDSDataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        pickle_path_train,\n        pickle_path_test,\n        batch_size: int = BATCH_SIZE,\n        val_split=0.2,\n        num_workers: int = NUM_WORKERS\n    ):\n        super().__init__()\n        self.pickle_path_train = pickle_path_train\n        self.pickle_path_test =  pickle_path_test #initalize class data with dataset\n        self.batch_size = batch_size #initalize class data batch size\n        self.val_split = val_split\n        self.num_workers = num_workers #assigns cpu cores\n\n        #defines class tranformation methods\n        #Tensors are similar to NumPy arrays, but tensors have accelerator support. \n        #Tensors are optimized for automatic differentiation.\n        \"\"\"\n        self.transform = transforms.Compose(\n            [\n                #transforms.Resize((224,224)), # img_size=224 is a convention rooted in ImageNet-pretrained models. Safe default or standard\n                #transforms.ToTensor(), transforms images to tensors.\n                transforms.Normalize(mean=[0.4234, 0.4927, 0.4830], std=[0.1923, 0.1815, 0.1827]) #tranformed data is normalized. Ex: 0.1307 = mean; 0.3081 = std deviation\n            ]\n        )\n        \"\"\"\n        self.transform = transforms.Normalize(mean=[0.4234, 0.4927, 0.4830], std=[0.1923, 0.1815, 0.1827])\n        self.dims = (1, 28, 28)\n        self.num_classes = 10\n\n        #def prepare_data(self):\n        # download\n        #MNIST(self.data_dir, train=True, download=True) #download training data\n        #MNIST(self.data_dir, train=False, download=True)#download testing data\n\n    # Assign train/val datasets for use in dataloaders\n    def setup(self, stage=None):\n        \n        if stage == \"fit\" or stage is None:\n            # Assign train/val datasets for use in dataloaders\n            dataset = PickleImageDataset(self.pickle_path_train, transform=self.transform)\n            val_size = int(len(dataset) * self.val_split)\n            train_size = len(dataset) - val_size\n            self.train_dataset, self.val_dataset = random_split(dataset, [train_size, val_size])\n            \n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            dataset = PickleImageDataset(self.pickle_path_test, transform=self.transform)\n            val_size = 0\n            test_size = len(dataset) - val_size\n            self.test_dataset, x = random_split(dataset, [test_size, val_size])\n\n    #Training\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n        )\n    #Validation\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n    #Testing\n    def test_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.58Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_mean_std(pickle_path, batch_size=64, img_size=224):\n    transform = transforms.Compose([\n        transforms.Resize((img_size, img_size)),\n        transforms.ToTensor()\n    ])\n\n    #dataset = datasets.ImageFolder(data_dir, transform=transform)\n    dataset = PickleImageDataset(pickle_path)\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n    mean = 0\n    std = 0\n    total_images_count = 0\n\n    for images in tqdm(loader):\n        if images is None:\n            continue\n        #images = batch[0] if isinstance(batch, (list, tuple)) else batch # support both (data, labels) & data only\n        batch_samples = images.size(0) # batch size (number of images)\n        images = images.view(batch_samples, images.size(1), -1) #[B, C, H*W]\n        mean += images.mean(2).sum(0)\n        std += images.std(2).sum(0)\n        total_images_count += batch_samples\n    mean /= total_images_count\n    std /= total_images_count\n    return mean, std","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.58Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mean, std = get_mean_std(trainingDataPath)\nprint(\"mean is: \", mean)\nprint(\"std dev is: \", std)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.58Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### BUILD THE GENERATOR\n#### Generate fake data from random noise","metadata":{}},{"cell_type":"code","source":"\"\"\"\n# Generate Fake Data: output like real data [1, 28, 28] and values -1, 1\nclass Generator(nn.Module):\n    def __init__(self, latent_dim):\n        super().__init__()\n        self.lin1 = nn.Linear(latent_dim, 7*7*64)  # [n, 256, 7, 7]\n        self.ct1 = nn.ConvTranspose2d(64, 32, 4, stride=2) # [n, 64, 16, 16]\n        self.ct2 = nn.ConvTranspose2d(32, 16, 4, stride=2) # [n, 16, 34, 34]\n        self.conv = nn.Conv2d(16, 1, kernel_size=7)  # [n, 1, 28, 28]\n    \n\n    def forward(self, x):\n        # Pass latent space input into linear layer and reshape\n        x = self.lin1(x)\n        x = F.relu(x)\n        x = x.view(-1, 64, 7, 7)  #256\n        \n        # Upsample (transposed conv) 16x16 (64 feature maps)\n        x = self.ct1(x)\n        x = F.relu(x)\n        \n        # Upsample to 34x34 (16 feature maps)\n        x = self.ct2(x)\n        x = F.relu(x)\n        \n        # Convolution to 28x28 (1 feature map)\n        return self.conv(x)\n\"\"\"\n\nclass Generator(nn.Module):\n    def __init__(self, latent_dim):\n        super().__init__()\n        self.latent_dim = latent_dim\n        \n        self.fc = nn.Linear(latent_dim, 512 * 4 * 4)\n        \n        self.deconv_layers = nn.Sequential(\n            nn.ConvTranspose2d(512, 256, 4, 2, 1),  # 4 -> 8\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            \n            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # 8 -> 16\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            \n            nn.ConvTranspose2d(128, 64, 4, 2, 1),   # 16 -> 32\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            \n            nn.ConvTranspose2d(64, 3, 4, 2, 1),     # 32 -> 64\n            nn.Tanh()  # final activation for output in [-1, 1]\n        )\n\n    def forward(self, z):\n        x = self.fc(z)\n        x = x.view(-1, 512, 4, 4)\n        x = self.deconv_layers(x)\n        return x\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.58Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### BUILD THE DISCRIMINATOR\n#### Distinguish between real and fake data","metadata":{}},{"cell_type":"code","source":"\"\"\"\n# Detective: fake or no fake -> 1 output [0, 1]\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Simple CNN\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)# 1 = grayscale or channels in input image, 10 filters of size(5,5) \n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 1)\n  \n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        # Flatten the tensor so it can be fed into the FC layers\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return torch.sigmoid(x)\n\"\"\"\n\n\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 64, 4, 2, 1),  # 64x64 -> 32x32\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(64, 128, 4, 2, 1),  # 32x32 -> 16x16\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(128, 256, 4, 2, 1),  # 16x16 -> 8x8\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(256, 512, 4, 2, 1),  # 8x8 -> 4x4\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n        self.fc = nn.Linear(512 * 4 * 4, 1)\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = x.view(-1, 512 * 4 * 4)\n        x = self.fc(x)\n        return x  # raw logits — use BCEWithLogitsLoss for training\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.58Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#TODO: GAN\nclass GAN(pl.LightningModule):\n    def __init__(self, latent_dim=100, lr=0.0002):\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.generator = Generator(latent_dim=self.hparams.latent_dim)\n        self.discriminator = Discriminator()\n        #random noise 6 images\n        self.validation_z = torch.randn(6, self.hparams.latent_dim)\n        self.automatic_optimization = False\n\n    def forward(self, z):\n        return self.generator(z)\n\n    def adversarial_loss(self, y_hat, y):\n        y_hat = torch.sigmoid(y_hat)\n        return F.binary_cross_entropy(y_hat, y)\n    \"\"\"\n    def training_step(self, batch, batch_idx):\n        real_imgs, _ = batch\n        # sample noise\n        z = torch.randn(real_imgs.shape[0], self.hparams.latent_dim)\n        z = z.type_as(real_imgs)\n        opt_g, opt_d = self.optimizers()\n        #train generator: max log(D(G(z)))\n        #if optimizer_idx == 0:\n        fake_imgs = self(z) #generator generates fake images\n        y_hat = self.discriminator(fake_imgs) #model prediction by discriminator of fake images\n\n        y = torch.ones(real_imgs.size(0), 1)\n        y = y.type_as(real_imgs) # move to gpu\n        g_loss = self.adversarial_loss(y_hat, y)\n        log_dict = {\"g_loss\": g_loss}\n        #return {\"gloss\": g_loss, \"progress_bar\": log_dict, \"log\": log_dict}\n\n        #train discriminator: max log (D(x) + log(1 - D(G(z))))\n        #if optimizer_idx == 1:\n        # how well can it label as real\n        y_hat_real = self.discriminator(real_imgs) #model prediction by discriminator of real images\n        y_real = torch.ones(real_imgs.size(0), 1) #initialize tensors of unity of size equal to batch size (number of images-->.size(0))\n        y_real = y_real.type_as(real_imgs) # move to gpu\n        real_loss = self.adversarial_loss(y_hat_real, y_hat)\n        # how well can it label as fake\n        y_hat_fake = self.discriminator(self(z).detach()) #model prediction by discriminator of fake images. self(z) generates fake images\n        #.detach() will create a new tensor which is detached from computational graph\n        y_fake = torch.zeros(real_imgs.size(0), 1) #initialize tensors of zero of size equal to batch size (number of images-->.size(0))to implement 1 - D(G(z)))\n        y_fake = y_fake.type_as(real_imgs) # move to gpu\n        fake_loss = self.adversarial_loss(y_hat_fake, y_fake)\n        d_loss = (real_loss + fake_loss) / 2\n\n        log_dict = {\"d_loss\": d_loss}\n        return {\"dloss\": d_loss, \"progress_bar\": log_dict, \"log\": log_dict}\n    \"\"\"\n    def training_step(self, batch, batch_idx):\n        #print(f\"Batch content: {type(batch)}, {len(batch)}\")\n        #print(batch)\n        #real_imgs, _ = batch\n        real_imgs = batch\n        z = torch.randn(real_imgs.shape[0], self.hparams.latent_dim, device=self.device)\n        opt_g, opt_d = self.optimizers()\n    \n        # === Train Generator ===\n        fake_imgs = self(z) # Generate fake images from random noise\n        y_hat_fake = self.discriminator(fake_imgs) # Discriminator's prediction on these fake images\n        valid = torch.ones(real_imgs.size(0), 1, device=self.device) # The generator wants the discriminator to think these are real:\n        g_loss = self.adversarial_loss(y_hat_fake, valid) # Adversarial loss: how well did G fool D?\n        # backpropagate g_loss and step the Generator’s optimizer\n        opt_g.zero_grad()\n        self.manual_backward(g_loss)\n        opt_g.step()\n        # The Generator learns by backpropagating the loss computed from how well it fools the Discriminator\n        # === Train Discriminator ===\n        y_hat_real = self.discriminator(real_imgs)\n        real_loss = self.adversarial_loss(y_hat_real, valid)\n    \n        fake_imgs_detached = fake_imgs.detach()\n        y_hat_fake_detached = self.discriminator(fake_imgs_detached)\n        fake = torch.zeros(real_imgs.size(0), 1, device=self.device)\n        fake_loss = self.adversarial_loss(y_hat_fake_detached, fake)\n    \n        d_loss = (real_loss + fake_loss) / 2\n    \n        opt_d.zero_grad()\n        self.manual_backward(d_loss)\n        opt_d.step()\n    \n        # Logging\n        self.log(\"g_loss\", g_loss, prog_bar=True, on_step=True, on_epoch=True)\n        self.log(\"d_loss\", d_loss, prog_bar=True, on_step=True, on_epoch=True)\n\n\n\n    \n    \n    def configure_optimizers(self):\n        lr = self.hparams.lr\n        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr)\n        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr)\n        return [opt_g, opt_d], []\n    \"\"\"\n    def plot_imgs(self):\n        z = self.validation_z.type_as(self.generator.lin1.weight)\n        sample_imgs = self(z).cpu()\n        print('epoch', self.current_epoch)\n        fig = plt.figure()\n        for i in range(sample_imgs.size(0)):\n            plt.subplot(2,3,i+1)\n            plt.tight_layout()\n            plt.imshow(sample_imgs.detach()[i, 0, :, :], cmap='gray_r', interpolation='none')\n            #plt.imshow(np.transpose(sample_imgs.detach()[i].cpu().numpy(), (1, 2, 0)))\n            plt.title(\"Generated Data\")\n            plt.xticks([])\n            plt.yticks([])\n            plt.axis('off')\n        plt.show()\n    \"\"\"\n    def plot_imgs(self):\n        z = self.validation_z.to(self.device)\n        sample_imgs = self(z).cpu()\n        print('epoch', self.current_epoch)\n        fig = plt.figure()\n        for i in range(sample_imgs.size(0)):\n            plt.subplot(2, 3, i+1)\n            plt.tight_layout()\n            # Assuming RGB images:\n            img_np = np.transpose(sample_imgs.detach()[i].numpy(), (1, 2, 0))\n            plt.imshow(img_np)\n            plt.title(\"Generated Data\")\n            plt.axis('off')\n        plt.show()\n\n    \"\"\"\n    def on_epoch_end(self):\n        self.plot_imgs()\n    \"\"\"\n    def on_train_epoch_end(self):\n        self.plot_imgs()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.58Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dm = BIRDSDataModule(trainingDataPath, testindDataPath)\nmodel = GAN()\nmodel.plot_imgs()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.58Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = pl.Trainer(max_epochs=200)\ntrainer.fit(model, dm)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-11T05:34:30.58Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ","metadata":{}}]}